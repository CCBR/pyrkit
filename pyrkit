#!/usr/bin/env bash

# pyrkit: a tool to archive and co-locate NGS data with hierarchical metadata
set -euo pipefail

VERSION="0.1.0-beta"

# Functions
function err() { cat <<< "$@" 1>&2; }
function fatal() { cat <<< "$@" 1>&2; exit 1; }
function log() { echo -e "[$(date +'%Y-%m-%dT%H:%M:%S%z')]: $@"; }
function abspath() { readlink -e "$1"; }
function parser() {
# Parses command-line args using argparse.bash
# @INPUT "$@" = user command-line arguments

ARGPARSE_DESCRIPTION="A tool to archive and co-locate NGS data with structured metadata"
ARGPARSE_EPILOG="Version $VERSION"
argparse "$@" << EOF || exit 0
# Required arguments group
required = parser.add_argument_group('Required Arguments')
optional = parser.add_argument_group('Optional Arguments')
required.add_argument('-i', '--input-directory', required=True, type=str,
                    help='Required local directory containing files (raw data and output files) \
                    to upload into object storage (HPC DME). This path is the output directory \
                    of a pipeline. Example: -i /data/projects/ccbr123/RNA/')
required.add_argument('-o', '--output-vault', required=True, type=str,
                    help='Required vault in HPC DME to upload and archive local \
                    input files and metadata. This vault represents the root HPC DME \
                    path to archive the data located in --input-directory into object \
                    storage. CCBR has two main vaults: /CCBR_EXT_Archive and /CCBR_Archive. \
                    /CCBR_EXT_Archive is for storing any public data such as data or results \
                    from dbGap, SRA, GEO or EBI. /CCBR_Archive is for storing any other data \
                    such as data from internal (SF) and external sequencing providers \
                    (NovoGene, GeneDx, Macrogen, Genentech). Example: -o /CCBR_Archive')
required.add_argument('-r', '--request-template', required=True, type=str,
                    help='Required Project Request Template. The project request template is \
                    an excel spreadsheet sent out to the requestor to capture information about \
                    a project or experiment. This excel file is parsed to capture any required \
                    metadata for instantiating a PI-, Project-, and Sample-level collection hierarchy \
                    in HPC DME. Example: -t experiment_metadata.xlsx')
required.add_argument('-m', '--multiqc-directory', required=True, type=str,
                    help='Required MultiQC Output Directory. This directory is created by \
                    MultiQC. It contains several text files which are generated by MultiQC \
                    as it builds the report. These files are parsed to \
                    attach quality-control metadata to each Sample-level collection. \
                    Example: -m /data/projects/ccbr123/RNA/multiqc_data/')
# Optional arguments group
optional.add_argument('-p', '--project-id', type=str,
                    help='Optional Project ID. This is a unique identifer or alias tied to \
                    a request to internally distinguish a project. This could be a CCBR/NCBR/NAS project \
                    ID. Example: -p ccbr-123')
optional.add_argument('-h', '--help', action='help', default=argparse.SUPPRESS,
                    help='Display help message and exit')
optional.add_argument('--version', action='version',
                    version='%(prog)s $VERSION', help="Display version information and exit")
EOF
}


function provided() {
  # Checks to see if key,value pairs exist
  # @INPUT $1 = name of user provided argument
  # @INPUT $2 = value of user provided argument
  # @CALLS fatal() if value is empty string or NULL

  if [[ -z "${2:-}" ]]; then
     fatal "Fatal: Failed to provide value to '${1}'!";
  fi
}


function clean(){
  # Finds the base name of the sample
  # @INPUT $1 = From optional basename argument
  # @RETURN $bname = cleaned base name (PATH and EXT removed)

  local bname=${1:-}
  local exts=("_1.fastq" "_2.fastq" ".R1.fastq" ".R2.fastq" "_R1.fastq" "_R2.fastq")

  if [[ -z "$bname" ]]; then
     bname="${Arguments[r1]}"  # Determine base name from R1 input
  fi

  # Remove PATH and .gz extension
  bname=$(basename $bname | sed 's/.gz$//g')

  # Clean remaining extensions (MateInfo + )
  for ext in "${exts[@]}"; do
    if [[ $bname == *${ext} ]]; then
      bname=$(echo "$bname" | sed "s@$ext\$@@")
      break # only remove one extension
    fi
  done

  echo "$bname"
}


function require(){
  # Requires an executable is in $PATH, as a last resort it will attempt to load
  # the executable or dependency as a module
  # @INPUT $@ = List of dependencies or executables to check

  for exe in "${@}"; do
    # Check if executable is in $PATH
    command -V ${exe} &> /dev/null && continue;
    # Try to load exe as lua module
    module load ${exe} &> /dev/null || \
      fatal "Failed to find or load '${exe}', not installed on target system."
  done
}


function init(){
  # Intializes a list of directories
  # @INPUT $@ = List of directories

  for d in "${@}"; do
    mkdir -p ${d} || fatal "Failed to initialize '${d}' directory";
  done
}


function lint(){
  # Lints user-provided project request template (experiment_metadata.xlsx)
  # Check for basic errors and if user provided all the require metadata
  # See --request-template or $REQUEST_TEMPLATE in help for more information
  # @INPUT $1 = PATH to pyrkit/src/lint.py program
  # @INPUT $2 = Project Request Template files to parse and lint
  # @INPUT $3 = Output Directory
  python ${1} ${2} ${3}
}


function _get_inner_distances(){
  # Parses Inner Distance Maxmia from RSeQC output files
  # @INPUT $1 = Input Directory or pipeline working directory (i.e. $INPUT_DIRECTORY)
  # @INPUT $2 = MultiQC Output Directory
  # @INPUT $3 = Parsed Inner Distance Output file

  # Performant approach to checking if glob exists using a Bash built-in function
  if compgen -G "${1}/STAR_files/*.Aligned.sortedByCoord.out.inner_distance_freq.txt" > /dev/null; then
    echo -e "Sample\tInner_Dist_Maxima" > ${2}/${3} || fatal "Failed to write to MultiQC Directory ${2}"
    # Get Inner Distance Maxima from RSeQC Output files
    for f in ${1}/STAR_files/*.Aligned.sortedByCoord.out.inner_distance_freq.txt; do
      sample=$(basename $f | sed 's/^output.//' | sed 's/.p2.Aligned.sortedByCoord.out.inner_distance_freq.txt$//');
      inner_dist_maxima=$(sort -k3,3nr "$f" | awk -F '\t' 'NR==1{print $1}');
      echo -e "${sample}\t${inner_dist_maxima}";
    done >> ${2}/${3}
  fi
}


function _get_median_tin(){
  # Parses RSeQC Median TIN (transcript integrity number) from output files
  # @INPUT $1 = Input Directory or pipeline working directory (i.e. $INPUT_DIRECTORY)
  # @INPUT $2 = MultiQC Output Directory
  # @INPUT $3 = Parsed medTIN Output file

  # Performant approach to checking if glob exists using a Bash built-in function
  if compgen -G "${1}/STAR_files/*.summary.txt" > /dev/null; then
    echo -e "Sample\tmedian_tin" > ${2}/${3} || fatal "Failed to write to MultiQC Directory ${2}"
    # Parse medTIN from RSeQC tin.py output files
    cut -f1,3 "${1}"/STAR_files/*.summary.txt | \
      grep -v '^Bam_file' | \
      sed 's/.p2.Aligned.sortedByCoord.out.dmark.bam//g' | \
      awk -F '\t' '{printf "%s\t%.3f\n", $1,$2}' >> ${2}/${3}
  fi
}


function _get_flowcell_lanes(){
  # Parses Logfile with Information about each FastQ file
  # @INPUT $1 = Input Directory or pipeline working directory (i.e. $INPUT_DIRECTORY)
  # @INPUT $2 = MultiQC Output Directory
  # @INPUT $3 = Parsed Flowcell Lanes Output file

  # Performant approach to checking if glob exists using a Bash built-in function
  if compgen -G "${1}/rpt.on.fastqs.txt" > /dev/null; then
    echo -e "Sample\tflowcell_lanes" > ${2}/${3} || fatal "Failed to write to MultiQC Directory ${2}"
    # Parse Flowcell Lane Information
    sed 's/\.R1\.fastq\.gz//g' "${1}/rpt.on.fastqs.txt" | \
      awk -v OFS='\t' '{print $1,$NF}' >> ${2}/${3}
  fi
}


function _get_sample_groups(){
  # Parses sample group information from sample.json (generated by running lint)
  # @INPUT $1 = Input Directory containing sample.json generated by @init()
  # @INPUT $2 = MultiQC Output Directory
  # @INPUT $3 = Parsed Sample Group Output file

  # Performant approach to checking if glob exists using a Bash built-in function
  if compgen -G "${1}/sample.json" > /dev/null; then
    echo -e "Sample\tTissueType"  > ${2}/${3} || fatal "Failed to write to MultiQC Directory ${2}"
    # Parse Group informaion from sample.json with jq
    jq -r 'to_entries[] | [.value."Sample Name", .value.Group] | @tsv' "${1}/sample.json" \
      | sed 's/\tnan$/\tUnknown/g' >> ${2}/${3}
  fi

}


function parse(){
  # Parse additional output files that MultiQC does not automatically parse
  # @INPUT $1 = Input Directory or pipeline working directory (i.e. $INPUT_DIRECTORY)
  # @INPUT $2 = MultiQC Directory (i.e. $MULTIQC_DIRECTORY)

  # Parse Addtional QC metadata from logfiles
  _get_inner_distances "${1}" "${2}" "mqc_rseqc_inner_distance_plot_Percentages_parsed.txt"
  _get_median_tin "${1}" "${2}" "rseqc_median_tin.txt"
  _get_flowcell_lanes "${1}" "${2}" "sample_flowcell_lanes.txt"
  _get_sample_groups "${1}/DME" "${2}" "sample_group.txt"
}


function QC(){
  # Aggregates MultiQC information across all samples and generates a QC Table
  # @INPUT $1 = PATH to pyrkit/src/pyparser.py program
  # @INPUT $2 = MultiQC Directory (i.e. $MULTIQC_DIRECTORY)

  # Run in sub-shell to keep current working diretory
  (cd "${2}"; python "${1}" "${2}"/*.txt )
}


function fingerprint(){
  # Generates a Unique Identifer for an Analysis
  # Analysis ID is determinstic and based on user inputs to pipeline
  # @INPUT $1 = Input Directory or pipeline working directory (i.e. $INPUT_DIRECTORY)
  # @INPUT $2 = DME base directory for all intermediate output files
  # @RETURNS inputs_md5, analysis_id

  # runinfo.txt aggregates all important user inputs, fix inconsistent delimiter
  sed 's/^file /file\t/g' "${1}/runinfo.txt" > "${2}/run_metadata.txt"

  # Convert Input Files to MD5 checksums
  while read field value; do
    # Get MD5 checksum if evaluating an input file
    ifile=$(if [[ -f "$value" ]]; then md5sum "$value" | awk '{print $1}'; else echo "$value"; fi)
    echo -e "$field\t$ifile"
  done < <(grep -v '^jobname\|^gtf_ver\|^assembly_name\|^species' "${2}/run_metadata.txt" | sort -k2,2) \
  > ${2}/run_inputs.md5

  # Add MD5 checksum of all inputs and create an analysis id to run metadata
  inputs_md5=$(md5sum ${2}/run_inputs.md5 | awk '{print $1}')
  analysis_id="${inputs_md5:0:3}-${inputs_md5:${#inputs_md5}/2:2}-${inputs_md5:${#inputs_md5}-4}"
  echo -e "md5_all_inputs\t${inputs_md5}" >> "${2}/run_metadata.txt"
  echo -e "md5_all_inputs_serial\t${analysis_id}" >> "${2}/run_metadata.txt"

  # Return inputs_md5, analysis_id
  echo -e "${inputs_md5}\t${analysis_id}"
}

function main(){
  # Parses cli args using argparse.bash and input files to extract metadata for upload
  # Initializes upload directory representing DME heirarchy
  # Generates collection and dataobject metadata based on inputs
  # @INPUT "$@" = command-line arguments
  # @CALLS require(), parser(), init(), lint(), parse(), QC(), fingerprint()
  # @EXPORTED ARGPARSE VARIABLES:
  #  - "Input Directory: $INPUT_DIRECTORY"
  #  - "Output DME Vault: $OUTPUT_VAULT"
  #  - "Request Template: $REQUEST_TEMPLATE"
  #  - "MultiQC HOME: $MULTIQC_DIRECTORY"
  #  - "Project ID: $PROJECT_ID"

  # Check system dependencies are installed
  require git jq python/3.5

  # Check for version flag
  case ${1:-} in
    --version) echo "$(basename $0) $VERSION" && exit 0;;
  esac

  # pyrkit home directory or installation location
  repohome=$(abspath $(dirname  "$0"))

  # Enable argparse parsing within bash
  source $(dirname $0)/src/argparse.bash || \
    fatal "Fatal: Failed to locate argparse.bash in ${repohome}!"

  # Parse command-line arguments with argparse
  if [ $# -eq 0 ]; then parser -h; fi  # Display usage, user did not provide any args
  parser "${@}"

  # Base directory for all intermediate output files
  output="${INPUT_DIRECTORY%/}/DME"

  # Initialize output diretory, lint template, parse logs, and aggregate QC information
  init "${output}"
  lint "${repohome}/src/lint.py" "${REQUEST_TEMPLATE}" "${output}"
  parse "${INPUT_DIRECTORY%/}" "${MULTIQC_DIRECTORY%/}"
  QC "${repohome}/src/pyparser.py" "${MULTIQC_DIRECTORY%/}"

  # Generate unique and determinstic Analysis ID based on User Inputs
  local inputs_md5
  local analysis_id
  read inputs_md5 analysis_id < <(fingerprint "${INPUT_DIRECTORY%/}" "${output}")
}

# Main: check usage, parse args, extract metadata from inputs and generate upload heirarchy
main "$@"
